{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authors\n",
    " - Nwamaka Nzeocha\n",
    " - Fabian Okeke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- [MovieLens 10M data set](http://grouplens.org/datasets/movielens/10m/)\n",
    "- [MovieLens 22M data set](http://grouplens.org/datasets/movielens/latest/)\n",
    "- [Million song data set](http://labrosa.ee.columbia.edu/millionsong/tasteprofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended settings\n",
    " - MovieLens 10M dataset extracted will lead to **ml-10M100K/ratings.dat**\n",
    " - MovieLens 22M dataset isn't used in this project\n",
    " - Song dataset extracted will lead to the file: **train_triplets.txt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======== PART 1 ========\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "from math import sqrt \n",
    "from operator import add\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel\n",
    "\n",
    "\n",
    "def exists(filepath):\n",
    "    return os.path.exists(filepath)\n",
    "\n",
    "\n",
    "def parse_rating(line):\n",
    "    \"\"\"\n",
    "    Parses a rating record that's in MovieLens format.\n",
    "    \n",
    "    :param str line: userId::movieId::rating::timestamp\n",
    "    \"\"\"\n",
    "    fields = line.strip().split(\"::\")\n",
    "\n",
    "    return (int(fields[0]),   # User ID\n",
    "            int(fields[1]),   # Movie ID\n",
    "            float(fields[2])) # Rating\n",
    "\n",
    "\n",
    "def compute_rmse(model, data, dataCount, bias=None):\n",
    "    \"\"\"\n",
    "        Compute RMSE (Root Mean Squared Error).\n",
    "        if bias is set then add bias to predictions\n",
    "    \"\"\"\n",
    "    predictions = model.predictAll(data.map(lambda x: (x[0], x[1]))) #userId and #movieId\n",
    "\n",
    "    if type(bias) == float:\n",
    "        predictions = predictions.map(lambda(u,m,r): (u,m,r+bias))\n",
    "    elif type(bias) == dict:\n",
    "        predictions = predictions.map(lambda(u,m,r): (u,m,r+bias.get(0,)))\n",
    "        \n",
    "    predictionsAndRatings = \\\n",
    "        predictions.map(lambda x: ((x[0], x[1]), x[2])) \\\n",
    "                   .join(data.map(lambda x: ((x[0], x[1]), x[2]))) \\\n",
    "                   .values()\n",
    "    return sqrt(\n",
    "        predictionsAndRatings.map(\n",
    "            lambda x: (x[0] - x[1]) ** 2\n",
    "        ).reduce(add) / float(dataCount)\n",
    "    )\n",
    "\n",
    "\n",
    "def getBestTrainingParameters(training, validation, validationCount, biasDict=None, isImplicit=False):\n",
    "    \"\"\"\n",
    "        Train ALS model using different regularization parameter and latent factors\n",
    "    \"\"\" \n",
    "\n",
    "    # NB: longer the list the longer the time spent training\n",
    "    rank_list = [10, 20, 30] # latent factor \n",
    "    lamda_list = [0.01, 0.1, 1.0] # regularization parameter\n",
    "\n",
    "    iterations = 5\n",
    "    bestModel, bestRMSE, bestRank, bestLamda = None, float(\"inf\"), None, None\n",
    "\n",
    "    for rank in rank_list:\n",
    "        for lamda in lamda_list:\n",
    "            \n",
    "            if isImplicit:\n",
    "                model = ALS.trainImplicit(training, rank, iterations, lamda)\n",
    "            else:\n",
    "                model = ALS.train(training, rank, iterations, lamda)\n",
    "                \n",
    "            rmse = compute_rmse(model, validation, validationCount, biasDict)\n",
    "            if rmse < bestRMSE:\n",
    "                bestModel, bestRMSE, bestRank, bestLamda = model, rmse, rank, lamda\n",
    "\n",
    "            print 'RMSE={}: Rank={}, Lambda={}'.format(rmse, rank, lamda)\n",
    "    \n",
    "    return (bestModel,bestRMSE,bestRank,bestLamda)\n",
    "\n",
    "\n",
    "def generate_recommendations(model, ratingsFile, numOfRec=5, isRecommendingSongs=False):\n",
    "    \"\"\"\n",
    "         use a trained ALS model(explicit/implicit) and a ratingsFile to predict movies/songs for a user\n",
    "    \"\"\"\n",
    "    \n",
    "    recommendations = None\n",
    "    \n",
    "    if (isRecommendingSongs):\n",
    "        usedItems = sc.textFile(ratingsFile)\\\n",
    "            .map(parse_song)\\\n",
    "            .map(lambda x: x[1])\\\n",
    "            .collect()\n",
    "        usedItems = list(set(usedItems))\n",
    "        \n",
    "        unseenItems = sc.textFile('train_triplets.txt')\\\n",
    "            .map(parse_song)\\\n",
    "            .map(lambda x: (x[1], 1))\\\n",
    "            .reduceByKey(add)\\\n",
    "            .map(lambda x: x[0]) \\\n",
    "            .filter(lambda x: x not in usedItems)\\\n",
    "            .map(lambda x: (x, 1))\n",
    "            \n",
    "        predictions = model.predictAll(unseenItems)\n",
    "        predictions = predictions.top(numOfRec, key=lambda x: x[2]) # sort by desc playCount\n",
    "        \n",
    "        allSongKeys, allSongValues = allSongs.keys(), allSongs.values() # allSongValues, allSongKeys are global var\n",
    "        recommendations = [allSongKeys[allSongValues.index(r[0])] for r in predictions] # get str IDs from int IDs\n",
    "\n",
    "    else:\n",
    "        usedItems = sc.textFile(ratingsFile) \\\n",
    "            .filter(lambda x: x and len(x.split('::')) == 4) \\\n",
    "            .map(parse_rating) \\\n",
    "            .map(lambda x: x[1])\\\n",
    "            .collect()\n",
    "        usedItems = list(set(usedItems))\n",
    "\n",
    "        unseenItems = sc.textFile('ml-10M100K/ratings.dat')\\\n",
    "            .filter(lambda x: x and len(x.split('::')) == 4)\\\n",
    "            .map(parse_rating)\\\n",
    "            .map(lambda x: (x[1], 1))\\\n",
    "            .reduceByKey(add)\\\n",
    "            .map(lambda x: x[0]) \\\n",
    "            .filter(lambda x: x not in usedItems)\\\n",
    "            .map(lambda x: (x, 1))\n",
    "\n",
    "        predictions = model.predictAll(unseenItems)\n",
    "        predictions = predictions.top(numOfRec, key=lambda x: x[2]) # sort by desc ratings\n",
    "            \n",
    "        movies = ''\n",
    "        with open('ml-10M100K/movies.dat', 'r') as open_file:\n",
    "            movies = {int(line.split('::')[0]): line.split('::')[1]\n",
    "                  for line in open_file\n",
    "                  if len(line.split('::')) == 3}\n",
    "\n",
    "        recommendations = []\n",
    "        for movieId, _, _ in predictions:\n",
    "            if movieId in movies:\n",
    "                recommendations.append(movies[movieId])         \n",
    "        \n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split movie dataset into 60-20-20 train-validate-test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already created files: train60.dat, validation20.dat, test20.dat\n"
     ]
    }
   ],
   "source": [
    "if (exists('ml-10M100K/train60.dat') and exists('ml-10M100K/validation20.dat') and exists('ml-10M100K/test20.dat')):\n",
    "    print \"Already created files: train60.dat, validation20.dat, test20.dat\"    \n",
    "\n",
    "else:\n",
    "    # sort by timestamp (4th column)\n",
    "    print 'sorting file...'\n",
    "    !sort -t ':' -k4 ml-10M100K/ratings.dat > ml-10M100K/new_ratings.dat \n",
    "    print \"sorting complete.\"\n",
    "    \n",
    "    # split into 5 parts of 2 million each: train(3 parts), validation (1 part), test (1 part)\n",
    "    print \"splitting file...\"\n",
    "    !split -l 2000000 ml-10M100K/new_ratings.dat ff\n",
    "    !cat ffaa ffab ffac > ml-10M100K/train60.dat\n",
    "    !mv ffad ml-10M100K/validation20.dat\n",
    "    !mv ffae ml-10M100K/test20.dat\n",
    "    \n",
    "    # remove tmp files used to create partitions\n",
    "    !rm new_ratings.dat ff*\n",
    "    print \"splitting complete.\"    \n",
    "    print \"Newly created files: train60.dat, validation20.dat, test20.dat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load movie files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training = sc.textFile('ml-10M100K/train60.dat') \\\n",
    "         .filter(lambda x: x and len(x.split('::')) == 4) \\\n",
    "         .map(parse_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation = sc.textFile('ml-10M100K/validation20.dat') \\\n",
    "         .filter(lambda x: x and len(x.split('::')) == 4) \\\n",
    "         .map(parse_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = sc.textFile('ml-10M100K/test20.dat') \\\n",
    "         .filter(lambda x: x and len(x.split('::')) == 4) \\\n",
    "         .map(parse_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainCount = training.count()\n",
    "trainCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validationCount = validation.count()\n",
    "validationCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testCount = test.count()\n",
    "testCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show parts of contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(37746, 3409, 0.5), (37746, 175, 0.5), (51778, 5430, 0.5)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6352, 6787, 4.0), (26571, 1580, 4.0), (26571, 2115, 4.0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5337, 296, 4.0), (5337, 307, 4.0), (32329, 3745, 4.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create best ALS explicit model or load if already exists for movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chosenMovieModel since it exists.\n",
      "chosenMovieModel loaded.\n"
     ]
    }
   ],
   "source": [
    "chosenMovieModel, movieResults = None, None\n",
    "if exists(\"chosenMovieModel\"):\n",
    "    print \"Loading chosenMovieModel since it exists.\"\n",
    "    chosenMovieModel = MatrixFactorizationModel.load(sc, \"chosenMovieModel\")\n",
    "    print \"chosenMovieModel loaded.\"\n",
    "else:\n",
    "    movieResults = getBestTrainingParameters(chosenMovieModel, validation, validationCount)\n",
    "    chosenMovieModel = movieResults[0]\n",
    "    chosenMovieModel.save(sc, \"chosenMovieModel\")\n",
    "    print \"chosenMovieModel created\"\n",
    "    print \"movie best rmse:\", movieResults[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create ratings file that contains movie ratings for one user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User01Ratings.dat file already exists.\n"
     ]
    }
   ],
   "source": [
    "if exists('ml-10M100K/user01Ratings.dat'):\n",
    "    print \"User01Ratings.dat file already exists.\"\n",
    "else:\n",
    "    user01Ratings = sc.textFile('ml-10M100K/ratings.dat').filter(lambda x: x.split('::')[0] == '1') # userId == 1\n",
    "    user01Ratings.saveAsTextFile('ml-10M100K/user01Ratings.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate movie recommendations for a single user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Last House on the Left, The (1972)',\n",
       " 'Innocents, The (1961)',\n",
       " 'Seed of Chucky (2004)',\n",
       " 'Telling Lies in America (1997)',\n",
       " \"My Life and Times With Antonin Artaud (En compagnie d'Antonin Artaud) (1993)\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsFile = 'ml-10M100K/user01Ratings.dat'\n",
    "generate_recommendations(chosenMovieModel, ratingsFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======== PART 2 ========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Global Bias/User Bias/Item Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global average bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "globalAvg: 0.409168940991\n"
     ]
    }
   ],
   "source": [
    "sumCount = training.map(lambda (u,m,r): (m,r)).combineByKey(lambda value: (value, 1),\n",
    "                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "globalAvg = sumCount.map(lambda (label, (value_sum, count)): (label, round(value_sum / count, 3))) # 3 dp\n",
    "globalAvg = globalAvg.collectAsMap() # dict\n",
    "\n",
    "sumKeys, sumValues = 0, 0\n",
    "for k,v in enumerate(globalAvg):\n",
    "    sumKeys += k\n",
    "    sumValues += v\n",
    "\n",
    "globalAvg = float(sumKeys)/sumValues\n",
    "print \"globalAvg:\", globalAvg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove global average bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(37746, 3409, 0.09083105900871297),\n",
       " (37746, 175, 0.09083105900871297),\n",
       " (51778, 5430, 0.09083105900871297)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingWithoutGlobalAvg = training.map(lambda (u,m,r): (u,m,r-globalAvg))\n",
    "trainingWithoutGlobalAvg.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=1.05870964006: Rank=10, Lambda=0.01\n"
     ]
    }
   ],
   "source": [
    "globalAvgResults = getBestTrainingParameters(trainingWithoutGlobalAvg, validation, validationCount, globalAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse after treating avg ratings bias:  1.059\n"
     ]
    }
   ],
   "source": [
    "print \"rmse after treating avg ratings bias: {0: .3f}\".format(globalAvgResults[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Item/Movie bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sumCount = training.map(lambda (u,m,r): (m,r)).combineByKey(lambda value: (value, 1),\n",
    "                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "itemBias = sumCount.map(lambda (label, (value_sum, count)): (label, round(value_sum / count, 3))) # 3 dp\n",
    "itemBias = itemBias.collectAsMap() # dict\n",
    "\n",
    "# show n keys\n",
    "i, N = 0, 5\n",
    "for k,v in enumerate(itemBias):\n",
    "    print k, \":\", v\n",
    "    i += 1\n",
    "    if i == N: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove item bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingWithoutItemBias = training.map(lambda (u,m,r): (u,m,r-globalAvg[m]))\n",
    "itemBiasResults = getBestTrainingParameters(trainingWithoutItemBias, validation, validationCount, itemBias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"best rmse (item bias):\", itemBiasResults[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### User bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sumCount = training.map(lambda (u,m,r): (u,r)).combineByKey(lambda value: (value, 1),\n",
    "                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "userBias = sumCount.map(lambda (label, (value_sum, count)): (label, round(value_sum / count, 3))) # 3 dp\n",
    "userBias = userBias.collectAsMap() # dict\n",
    "\n",
    "# show n keys\n",
    "i, N = 0, 5\n",
    "for k,v in enumerate(userBias):\n",
    "    print k, \":\", v\n",
    "    i += 1\n",
    "    if i == N: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove user bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingWithoutUserBias = training.map(lambda (u,m,r): (u,m,userBias.get(u,0.409))) #replace with avg when no rating\n",
    "userBiasResults = getBestTrainingParameters(trainingWithoutUserBias, validation, validationCount, userBias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"best rmse (user bias):\", userBiasResults[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======== PART 3 ========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split song dataset into 60-20-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already created files: songTrain60.txt, songValidation20.txt, songTest20.txt\n"
     ]
    }
   ],
   "source": [
    "if (exists('songTrain60.txt') and exists('songValidation20.txt') and exists('songTest20.txt')):\n",
    "    print \"Already created files: songTrain60.txt, songValidation20.txt, songTest20.txt\"    \n",
    "\n",
    "else:\n",
    "    # split into chunks of 3.2 million each (total dataset: 48373586 lines)\n",
    "    print \"splitting file...\"\n",
    "    !split -l 3200000 train_triplets.txt ff\n",
    "    \n",
    "    !cat ffae ffaj ffab ffai ffaf ffad ffam ffac ffah > songTrain60.txt\n",
    "    !rm ffae ffaj ffab ffai ffaf ffad ffam ffac ffah\n",
    "    \n",
    "    !cat ffal ffag ffaa > songValidation20.txt\n",
    "    !rm ffal ffag ffaa\n",
    "    \n",
    "    !cat ff* > songTest20.txt\n",
    "    !rm ff*\n",
    "\n",
    "    print \"splitting complete.\"    \n",
    "    print \"Newly created files: songTrain60.txt, songValidation20.txt, songTest20.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosenSongModel loaded since it exists.\n"
     ]
    }
   ],
   "source": [
    "chosenSongModel, songResults = None, None\n",
    "if exists(\"chosenSongModel\"):\n",
    "    print \"chosenSongModel loaded since it exists.\"\n",
    "    chosenSongModel = MatrixFactorizationModel.load(sc, \"chosenSongModel\")\n",
    "else:\n",
    "    songResults = getBestTrainingParameters(binarySongs, songValidation, songValidationCount, isImplicit=True)\n",
    "    chosenSongModel = songResults[0]\n",
    "    chosenSongModel.save(sc, \"chosenSongModel\")\n",
    "    print \"chosenSongModel created\"\n",
    "    print \"songResults best rmse:\", songResults[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allUsers = {}\n",
    "allSongs = {}\n",
    "\n",
    "with open('train_triplets.txt') as f:\n",
    "    for line in f:\n",
    "        userId, songId, playCount = line.split()\n",
    "        allUsers[userId] = allUsers.get(userId, len(allUsers) + 1);\n",
    "        allSongs[songId] = allSongs.get(songId, len(allSongs) + 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1019318, 384546)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allUsers), len(allSongs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert str ids to int ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_song(line):\n",
    "    \"\"\"\n",
    "        convert all str input to integer values\n",
    "        every user gets an integer id and every song gets an integer id\n",
    "    \"\"\"\n",
    "    userId, songId, playCount = line.split(\"\\t\")\n",
    "    return (allUsers[userId], # user ID\n",
    "            allSongs[songId], # song ID\n",
    "            int(playCount))   # playcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songTraining = sc.textFile('songTrain60.txt').map(parse_song)\n",
    "songValidation = sc.textFile('songValidation20.txt').map(parse_song)\n",
    "songTest = sc.textFile('songTest20.txt').map(parse_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9600000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songValidationCount = songValidation.count()\n",
    "songValidationCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show parts of contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 1), (1, 2, 1), (1, 3, 1)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songTraining.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 1), (1, 2, 4), (1, 3, 6)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songValidation.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 1), (1, 2, 7), (1, 3, 1)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songTest.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert play count to binary ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 0), (1, 2, 0), (1, 3, 0)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarySongs = songTraining.map(lambda(uid,sid,pc): (uid,sid,1) if pc > 5 else (uid,sid,0)) #userId, songId, playCount\n",
    "binarySongs.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ALS implicit model  or load if already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosenSongModel loaded since it exists.\n"
     ]
    }
   ],
   "source": [
    "chosenSongModel, songResults = None, None\n",
    "if exists(\"chosenSongModel\"):\n",
    "    print \"chosenSongModel loaded since it exists.\"\n",
    "    chosenSongModel = MatrixFactorizationModel.load(sc, \"chosenSongModel\")\n",
    "else:\n",
    "    songResults = getBestTrainingParameters(binarySongs, songValidation, songValidationCount, isImplicit=True)\n",
    "    chosenSongModel = songResults[0]\n",
    "    chosenSongModel.save(sc, \"chosenSongModel\")\n",
    "    print \"chosenSongModel created\"\n",
    "    print \"songResults best rmse:\", songResults[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create single user ratings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user01Songs.txt already exists.\n"
     ]
    }
   ],
   "source": [
    "if exists('user01Songs.txt'):\n",
    "    print \"user01Songs.txt already exists.\"\n",
    "else:\n",
    "    user01Songs = sc.textFile('train_triplets.txt')  #(user, song, play count) \n",
    "    user01Songs = user01Songs.filter(lambda x: x.split('\\t')[0] == 'b80344d063b5ccb3212f76538f3d9e43d87dca9e') # userId\n",
    "    user01Songs.saveAsTextFile('user01Songs.txt')\n",
    "    print \"user01Songs.txt created.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate user song recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "songRatingsFile = 'user01Songs.txt'\n",
    "songsRecommended = generate_recommendations(chosenSongModel, songRatingsFile, isRecommendingSongs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The id of songs recommended: ['SOGOLRD12A6D4F89E9', 'SOGHNYT12A8C138561', 'SOLXDDC12A6701FBFD', 'SOMHHTD12A67ADE988', 'SOXBJFG12A8C141FE0']\n"
     ]
    }
   ],
   "source": [
    "print \"The id of songs recommended:\", songsRecommended"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
